### 비트
- 0과 1을 표현하는 가장 작은 정보 단위
- n비트는 $2^n$가지의 정보 표현 가능
- 일반적으로는 비트보다 더 큰 단위를 사용
  - 바이트(1byte = 8bit)
  - 킬로바이트(1kB = 1,000byte)
  - 메가바이트(1MB = 1,000kB)
  - 기가바이트(1GB = 1,000MB)
  - 테라바이트(1TB = 1,000GB)
  
### 워드
- CPU가 한 번에 처리할 수 있는 정보의 크기 단위
- 하프 워드: 워드의 절반 크기
- 풀 워드: 워드 크기
- 더블 워드: 워드의 두 배 크기

### 이진법
- 0과 1로 수를 표현하는 방법
- 음수를 표현할 때는 2의 보수를 사용
  - 2의 보수는 어떤 수를 그보다 큰 $2^n$에서 뺀 값
  - 혹은 0과 1을 모두 반전시킨 뒤 1을 더한 값
  - ex. $11_{(2)}$보다 큰 $2^n$은 $100_{(2)}$
   $100_{(2)}$ - $11_{(2)}$ = $01_{(2)}$
  - ex. $11_{(2)}$의 0과 1을 반전시키면 $00_{(2)}$
  여기에 1을 더하면 $01_{(2)}$
  - 이렇게 하더라도 음수, 양수 구별은 불가능 → 플래그라는 것을 통해 음수, 양수 정보를 표현

### 16진법
- 0부터 9, A부터 F(10~15)으로 수를 표현하는 방법
- 2진법으로 모든 것을 표현하면 너무 길어짐
- 따라서 컴퓨터의 데이터를 표현할 때 십육진법도 많이 사용

### 문자를 표현하기 위한 요소
- 문자 집합: 컴퓨터가 이해할 수 있는 문자의 모음
- 인코딩: 문자를 0과 1로 이루어진 문자 코드로 변환하는 과정
- 디코딩: 0과 1로 표현된 문자 코드를 문자로 변환하는 과정

### 아스키 코드
- 초창기 문자 집합 중 하나
- 7비트로 하나의 문자를 표현하고 1비트는 오류 검출을 위한 패리티 비트로 사용 → 아스키 코드는 총 8비트
- 128($2^7$)개의 문자 밖에 표현할 수 없음
- 확장 아스키 코드도 등장했지만 여전히 적은 수의 문자 밖에 표현할 수 없음
  
### 한글 인코딩의 특성
- 한글은 영어와 달리 여러 문자들의 조합이 하나의 문자가 됨
- 완성형 인코딩 방식과 조합형 인코딩 방식
  - 완성형 인코딩 예시: '강'이라는 글자 하나를 하나의 문자로 인코딩
  - 조합형 인코딩 예시: '강'이라는 글자를 'ㄱ','ㅏ','ㅇ'이라는 세 문자의 조합으로 인코딩

### 유니코드
- 통일된 문자 집합
- 여러 언어들과 특수 문자 등을 표시 가능
- 유니코드의 인코딩 방식
  - UTF: Unicode Transformation Format
  - utf-8: 가변 길이 인코딩 방식으로 1바이트~4바이트의 결과값을 가짐
  - 이외에도 utf-16, utf-32 등등